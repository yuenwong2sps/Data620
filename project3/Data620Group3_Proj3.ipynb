{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Product Co-Purchasing Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Team: Fireflies\n",
    "\n",
    "Members: Chunhui Zhu, John Wong and Chunmei Zhu\n",
    "\n",
    "Date: 2/25/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'amazon-meta.txt', 'Amazon0601.txt', 'Data620Group3_Proj3.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Raw Data - Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#variables\n",
    "id=[]\n",
    "title=[]\n",
    "\n",
    "id_str = \"\"\n",
    "asin_str = \"\"\n",
    "title_str = \"\"\n",
    "\n",
    "reviewer=[]\n",
    "count=0\n",
    "\n",
    "\n",
    "myfile = open(\"amazon-meta.txt\", encoding='utf-8' )\n",
    "\n",
    "#extract amazon meta without customer review\n",
    "output_file = open(\"amazon-small-meta.csv\", encoding='utf-8',  mode = \"w+\")\n",
    "\n",
    "#extract amazon review with product id\n",
    "output_file3 = open(\"amazon-reviewer.csv\", encoding='utf-8',  mode = \"w+\")\n",
    "     \n",
    "\n",
    "line = myfile.readline() \n",
    "    \n",
    "while line:\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    \n",
    "    asin_str = \"\"\n",
    "    title_str = \"\" #init value\n",
    "    group_str = \"\" #init value\n",
    "    categories_str = \"\" #init value\n",
    "        \n",
    "\n",
    "    if 'Id:' in line:\n",
    "\n",
    "        reviewer.clear()\n",
    "        \n",
    "        numOfId = line[3:].strip()\n",
    "        id_str = numOfId #id code as string\n",
    "    \n",
    "    \n",
    "        #start after Id line: read in parallel\n",
    "    \n",
    "        line = myfile.readline()\n",
    "    \n",
    "     \n",
    "        if 'ASIN:' in line:\n",
    "        \n",
    "            asin_str = line[5:].strip()\n",
    "    \n",
    "        line = myfile.readline()\n",
    "        \n",
    "        if 'title:' in line:\n",
    "        \n",
    "            name=line[8:].strip()\n",
    "            name = name.replace(',',' ')\n",
    "                        \n",
    "            title_str = name\n",
    "\n",
    "        line = myfile.readline()\n",
    "\n",
    "        if 'group:' in line:\n",
    "        \n",
    "            name=line[8:].strip()\n",
    "                        \n",
    "            group_str = name\n",
    "\n",
    "        \n",
    "        line = myfile.readline() #read after salesrank\n",
    "        \n",
    "        line = myfile.readline() #read after similar\n",
    "        \n",
    "        line = myfile.readline() \n",
    "        \n",
    "\n",
    "        if 'categories:' in line:\n",
    "        \n",
    "            name=line[14:].strip()\n",
    "            name = name.replace(',',' ')                        \n",
    "            categories_str = name\n",
    "\n",
    "\n",
    "        output_file.writelines((id_str + \",\" + asin_str + \",\" + title_str + \",\" + group_str + \",\" + categories_str + \"\\n\"))\n",
    "        \n",
    "           \n",
    "              \n",
    "    \n",
    "    #skipping line by while loop\n",
    "    line = myfile.readline()\n",
    "    if 'cutomer:' in line: #only extract customer line        \n",
    "        c_values = line.split(':')  \n",
    "        #['2002-5-13  cutomer',' A2IGOA66Y6O8TQ  rating',' 5  votes','   3  helpful','   2']\n",
    "                    \n",
    "        name=c_values[1].replace('rating','')\n",
    "        name = name.strip()\n",
    "        reviewer.append(name)\n",
    "        \n",
    "        if id_str == '0':\n",
    "            output_file3.writelines(\"1,\" + name + '\\n') #unknown bug in first output, use \"1,reviewer customer id\"\n",
    "        else:            \n",
    "            output_file3.writelines(id_str + \",\" + name + '\\n') #id,reviewer customer id\n",
    "             \n",
    "\n",
    "    \n",
    "output_file.close()\n",
    "\n",
    "output_file3.close()\n",
    "\n",
    "myfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'amazon-meta.txt', 'amazon-reviewer.csv', 'amazon-small-meta.csv', 'Amazon0601.txt', 'Data620Group3_Proj3.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#verify File creation\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Raw Data - Co-purchasing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are commmets rows at the top of the co-purchasing file and it is '\\t'.  we need to convert it to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myfile = open(\"Amazon0601.txt\", encoding='utf-8' )\n",
    "\n",
    "output_file = open(\"edges_product_id.csv\", encoding='utf-8',  mode = \"w+\")\n",
    "\n",
    "\n",
    "line = myfile.readline() \n",
    "    \n",
    "while line:\n",
    "    \n",
    "    if line[0] != '#':\n",
    "        output_file.writelines(line.replace('\\t',','))\n",
    "    \n",
    "    line = myfile.readline()\n",
    "    \n",
    "    \n",
    "myfile.close()\n",
    "output_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow down co-purchasing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are over 1.7M rows of co-purchasing data over 400k products, we need to narrow down for more meaningful analysis.\n",
    "\n",
    "We will focus on the top 100 most reviewed products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using Reducer code to aggregate the reviewer entries.\n",
    "\n",
    "count = 0\n",
    "perkeycount = 0\n",
    "oldKey = None\n",
    "\n",
    "myfile = open(\"amazon-reviewer.csv\", encoding='utf-8' )\n",
    "\n",
    "output_file = open(\"amazon-reviewerCountByProductID.csv\", encoding='utf-8',  mode = \"w+\")\n",
    "\n",
    "\n",
    "line = myfile.readline() \n",
    "\n",
    "\n",
    "line = myfile.readline() #skip header\n",
    "        \n",
    "while line:\n",
    "\n",
    "    data_mapped = line.strip().split(\",\")\n",
    "    if len(data_mapped) != 2:\n",
    "        # Something has gone wrong. Skip this line.\n",
    "        continue\n",
    "\n",
    "    thisKey, thisReviewer = data_mapped\n",
    "    \n",
    "    if oldKey and oldKey != thisKey:\n",
    "        output_file.writelines(str(oldKey) + \",\" + str(perkeycount) + \"\\n\")\n",
    "        perkeycount = 0\n",
    "\n",
    "    oldKey = thisKey\n",
    "    perkeycount += 1\n",
    "\n",
    "    line = myfile.readline()\n",
    "    \n",
    "if oldKey != None:\n",
    "        output_file.writelines(str(oldKey) + \",\" + str(perkeycount)  + \"\\n\")    \n",
    "\n",
    "myfile.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'amazon-meta.txt', 'amazon-reviewer.csv', 'amazon-reviewerCountByProductID.csv', 'amazon-small-meta.csv', 'Amazon0601.txt', 'Data620Group3_Proj3.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#verify File creation\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, we will create filter co-purchasing data and nodes id for the top 100 most reviewed products\n",
    "\n",
    "\n",
    "reviewer_count_path = \"amazon-reviewerCountByProductID.csv\"\n",
    "\n",
    "raw_edges_prodct_id_path = \"edges_product_id.csv\"\n",
    "\n",
    "amazon_small_meta_path = \"amazon-small-meta.csv\"\n",
    "\n",
    "top100_co_purchasing_savePath = \"top100_edges.csv\"\n",
    "\n",
    "top100_node_savePath = \"top100_nodes.csv\"\n",
    "\n",
    "\n",
    "###Create new co-purchasing list from top 100 reviewed product\n",
    "reviewer_count = pd.read_csv(reviewer_count_path, index_col=False)\n",
    "\n",
    "reviewer_count.columns = [\"prod_id\",\"review_count\"]\n",
    "\n",
    "top100_reviewer_count = reviewer_count.sort_values('review_count',ascending=False).head(n=100)\n",
    "\n",
    "\n",
    "raw_edges_prodct_id = pd.read_csv(raw_edges_prodct_id_path, index_col=False)\n",
    "\n",
    "raw_edges_prodct_id.columns = [\"FromID\",\"ToID\"]\n",
    "\n",
    "df_mergeFromNode = pd.merge(top100_reviewer_count, raw_edges_prodct_id, left_on = \"prod_id\", right_on = \"FromID\")\n",
    "\n",
    "\n",
    "df_mergeToNode = pd.merge(top100_reviewer_count, raw_edges_prodct_id, left_on = \"prod_id\", right_on = \"ToID\")\n",
    "\n",
    "df_new_coPurchasing = pd.concat([df_mergeFromNode, df_mergeToNode], ignore_index= True)\n",
    "\n",
    "#output the new co-purchasing\n",
    "\n",
    "df_new_edge = df_new_coPurchasing[['FromID','ToID']]\n",
    "\n",
    "df_new_edge.columns = ['Source','Target']\n",
    "\n",
    "df_new_edge.to_csv(top100_co_purchasing_savePath, index = None, header=True)\n",
    "\n",
    "###Create node file for new co-purchasing list\n",
    "\n",
    "amazon_small_meta =  pd.read_csv(amazon_small_meta_path, index_col=False)\n",
    "\n",
    "amazon_small_meta.columns = [\"prod_id\",\"ASIN\",\"title\",\"group\",\"category\"]\n",
    "\n",
    "in_fromid = set(df_new_coPurchasing['FromID'].tolist())\n",
    "\n",
    "in_toid = set(df_new_coPurchasing['ToID'].tolist())\n",
    "\n",
    "small_node_ids = in_fromid | in_toid\n",
    "\n",
    "list_new_co_purchasing_prod_id = pd.DataFrame({'prod_id':list(small_node_ids)})\n",
    "\n",
    "top100_nodes = pd.merge(list_new_co_purchasing_prod_id, amazon_small_meta, left_on = 'prod_id', right_on = 'prod_id')\n",
    "\n",
    "#output the new node\n",
    "\n",
    "top100_nodes_new = top100_nodes[['prod_id','title']]\n",
    " \n",
    "top100_nodes_new.columns = ['id','label']\n",
    "\n",
    "top100_nodes_new.to_csv(top100_node_savePath, index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'amazon-meta.txt', 'amazon-reviewer.csv', 'amazon-reviewerCountByProductID.csv', 'amazon-small-meta.csv', 'Amazon0601.txt', 'Data620Group3_Proj3.ipynb', 'edges_product_id.csv', 'top100_edges.csv', 'top100_nodes.csv']\n"
     ]
    }
   ],
   "source": [
    "#verify File creation\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
